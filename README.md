# ğŸš€ Production ML API Deployment with Docker & AWS EC2

This repository demonstrates how to **package a Machine Learning API using Docker** and **deploy it to AWS EC2 in production**.

The focus of this project is **production readiness**.

---

## ğŸ¯ What This Project Covers

- Dockerizing a FastAPI-based ML inference service
- Creating a **production Dockerfile**
- Building and tagging Docker images
- Pushing images to Docker Hub
- Deploying and running containers on **AWS EC2**
- Exposing the API securely over the internet

---

## ğŸ§  ML Application Overview

The API serves a simple **Linear Regression model** using `scikit-learn`.

- Input: numeric value
- Output: predicted value
- Purpose: demonstrate ML inference in a production environment

The ML logic is intentionally simple so the focus remains on **deployment and infrastructure**.

---

## ğŸ“ Project Structure

```

docker-ml-deployment/
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â””â”€â”€ README.md
```

---

## ğŸ³ Production Dockerfile

```dockerfile
FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY app.py .

EXPOSE 8000

CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "2"]
```

Why this Dockerfile is production-ready:

No live reload (--reload removed)

Application code copied into the image

Multiple workers for concurrency

Slim base image for smaller size

---

## ğŸ—ï¸ Build Production Image Locally
```
docker build -t ml-api:prod .
```

## ğŸ§ª Test Image Locally (Production Mode)
```
docker run -d --name ml-prod-test -p 8000:8000 ml-api:prod
```

Test:

```
curl http://localhost:8000/
```

Stop test container after verification.

## ğŸ“¦ Push Image to Docker Hub

```
docker login
docker tag ml-api:prod YOUR_DOCKERHUB_USERNAME/ml-api:v1
docker push YOUR_DOCKERHUB_USERNAME/ml-api:v1
```


This makes the image available for cloud deployment.

## â˜ï¸ Deploy to AWS EC2

1ï¸âƒ£ Launch EC2 Instance

Ubuntu 22.04 LTS

Instance type: t2.medium or t3.micro

Open ports:

SSH (22) â€“ your IP

TCP (8000) â€“ public access

2ï¸âƒ£ Connect to EC2
```
ssh -i ml-key.pem ubuntu@YOUR_EC2_PUBLIC_IP
```

3ï¸âƒ£ Install Docker on EC2
```
sudo apt-get update
sudo apt-get install -y docker.io
sudo systemctl start docker
sudo systemctl enable docker
sudo usermod -aG docker ubuntu
```

Re-login after adding user to docker group.

4ï¸âƒ£ Run the Production Container
```
docker pull YOUR_DOCKERHUB_USERNAME/ml-api:v1

docker run -d \
  --name ml-api-production \
  --restart unless-stopped \
  -p 8000:8000 \
  YOUR_DOCKERHUB_USERNAME/ml-api:v1
```

## ğŸŒ Access Deployed API

Root: http://YOUR_EC2_PUBLIC_IP:8000/

Swagger UI: http://YOUR_EC2_PUBLIC_IP:8000/docs

Prediction endpoint: /predict

Your ML API is now live on the internet ğŸš€


## ğŸ Summary

This project demonstrates an end-to-end production workflow:

ML API â†’ Docker image â†’ Docker Hub â†’ AWS EC2
